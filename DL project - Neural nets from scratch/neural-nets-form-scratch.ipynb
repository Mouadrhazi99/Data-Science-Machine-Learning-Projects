{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def ReLU(Z):\n    return np.maximum(Z,0)\n\ndef derivative_ReLU(Z):\n    return Z > 0\n\ndef softmax(Z):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    exp = np.exp(Z - np.max(Z)) #le np.max(Z) evite un overflow en diminuant le contenu de exp\n    return exp / exp.sum(axis=0)\n\ndef init_params(size):\n    W1 = np.random.rand(10,size) - 0.5\n    b1 = np.random.rand(10,1) - 0.5\n    W2 = np.random.rand(10,10) - 0.5\n    b2 = np.random.rand(10,1) - 0.5\n    return W1,b1,W2,b2\n\ndef forward_propagation(X,W1,b1,W2,b2):\n    Z1 = W1.dot(X) + b1 #10, m\n    A1 = ReLU(Z1) # 10,m\n    Z2 = W2.dot(A1) + b2 #10,m\n    A2 = softmax(Z2) #10,m\n    return Z1, A1, Z2, A2\n\ndef one_hot(Y):\n    ''' return an 0 vector with 1 only in the position correspondind to the value in Y'''\n    one_hot_Y = np.zeros((Y.max()+1,Y.size)) #si le chiffre le plus grand dans Y est 9 ca fait 10 lignes\n    one_hot_Y[Y,np.arange(Y.size)] = 1 # met un 1 en ligne Y[i] et en colonne i, change l'ordre mais pas le nombre\n    return one_hot_Y\n\ndef backward_propagation(X, Y, A1, A2, W2, Z1, m):\n    one_hot_Y = one_hot(Y)\n    dZ2 = 2*(A2 - one_hot_Y) #10,m\n    dW2 = 1/m * (dZ2.dot(A1.T)) # 10 , 10\n    db2 = 1/m * np.sum(dZ2,1) # 10, 1\n    dZ1 = W2.T.dot(dZ2)*derivative_ReLU(Z1) # 10, m\n    dW1 = 1/m * (dZ1.dot(X.T)) #10, 784\n    db1 = 1/m * np.sum(dZ1,1) # 10, 1\n\n    return dW1, db1, dW2, db2\n\ndef update_params(alpha, W1, b1, W2, b2, dW1, db1, dW2, db2):\n    W1 -= alpha * dW1\n    b1 -= alpha * np.reshape(db1, (10,1))\n    W2 -= alpha * dW2\n    b2 -= alpha * np.reshape(db2, (10,1))\n\n    return W1, b1, W2, b2\n\ndef get_predictions(A2):\n    return np.argmax(A2, 0)\n\ndef get_accuracy(predictions, Y):\n    return np.sum(predictions == Y)/Y.size\n\ndef gradient_descennt(X, Y, alpha, iterations):\n    size , m = X.shape\n\n    W1, b1, W2, b2 = init_params(size)\n    for i in range(iterations):\n        Z1, A1, Z2, A2 = forward_propagation(X, W1, b1, W2, b2)\n        dW1, db1, dW2, db2 = backward_propagation(X, Y, A1, A2, W2, Z1, m)\n\n        W1, b1, W2, b2 = update_params(alpha, W1, b1, W2, b2, dW1, db1, dW2, db2)   \n\n        if (i+1) % int(iterations/10) == 0:\n            print(f\"Iteration: {i+1} / {iterations}\")\n            prediction = get_predictions(A2)\n            print(f'{get_accuracy(prediction, Y):.3%}')\n    return W1, b1, W2, b2\n\ndef make_predictions(X, W1 ,b1, W2, b2):\n    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)\n    predictions = get_predictions(A2)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2023-01-21T13:14:33.236580Z","iopub.execute_input":"2023-01-21T13:14:33.236994Z","iopub.status.idle":"2023-01-21T13:14:33.258162Z","shell.execute_reply.started":"2023-01-21T13:14:33.236962Z","shell.execute_reply":"2023-01-21T13:14:33.256865Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nfrom keras.datasets import mnist\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-01-21T13:14:37.140749Z","iopub.execute_input":"2023-01-21T13:14:37.141153Z","iopub.status.idle":"2023-01-21T13:14:37.147169Z","shell.execute_reply.started":"2023-01-21T13:14:37.141120Z","shell.execute_reply":"2023-01-21T13:14:37.145871Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\nSCALE_FACTOR = 255 # TRES IMPORTANT SINON OVERFLOW SUR EXP\nWIDTH = X_train.shape[1]\nHEIGHT = X_train.shape[2]\nX_train = X_train.reshape(X_train.shape[0],WIDTH*HEIGHT).T / SCALE_FACTOR\nX_test = X_test.reshape(X_test.shape[0],WIDTH*HEIGHT).T  / SCALE_FACTOR\n\nW1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.15, 200)\nwith open(\"trained_params.pkl\",\"wb\") as dump_file:\n    pickle.dump((W1, b1, W2, b2),dump_file)\n\nwith open(\"trained_params.pkl\",\"rb\") as dump_file:\n    W1, b1, W2, b2=pickle.load(dump_file)\n#show_prediction(0,X_test, Y_test, W1, b1, W2, b2)\n#show_prediction(1,X_test, Y_test, W1, b1, W2, b2)\n#show_prediction(2,X_test, Y_test, W1, b1, W2, b2)\n#show_prediction(100,X_test, Y_test, W1, b1, W2, b2)\n#show_prediction(200,X_test, Y_test, W1, b1, W2, b2)","metadata":{"execution":{"iopub.status.busy":"2023-01-21T13:18:24.660771Z","iopub.execute_input":"2023-01-21T13:18:24.662041Z","iopub.status.idle":"2023-01-21T13:19:06.697044Z","shell.execute_reply.started":"2023-01-21T13:18:24.661978Z","shell.execute_reply":"2023-01-21T13:19:06.695335Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"Iteration: 20 / 200\n48.950%\nIteration: 40 / 200\n67.513%\nIteration: 60 / 200\n74.118%\nIteration: 80 / 200\n74.072%\nIteration: 100 / 200\n77.803%\nIteration: 120 / 200\n79.217%\nIteration: 140 / 200\n81.703%\nIteration: 160 / 200\n82.768%\nIteration: 180 / 200\n83.608%\nIteration: 200 / 200\n84.362%\n","output_type":"stream"}]}]}